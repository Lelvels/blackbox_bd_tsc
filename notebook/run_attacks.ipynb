{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ea58e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import yaml\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import umap\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from utils.constants import RESULT_DIR, SRC_DIR, SEED\n",
    "from utils.data_handler import get_tsc_train_dataset, preprocess_data, create_dataset\n",
    "from models.clf_wrapper import ClassifierWrapper\n",
    "from models.trigger_gen import TriggerGenerator\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb97757",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "c14824b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape before encoding: (3683,)\n",
      "y_test shape before encoding: (921,)\n",
      "Train data shape: (3683, 120, 8), Test data shape: (921, 120, 8)\n"
     ]
    }
   ],
   "source": [
    "# dataset_name = \"iAWE\"\n",
    "dataset_name = \"MotionSense\"\n",
    "data_ratio = 1.0\n",
    "\n",
    "# Attacker data\n",
    "x_train_atk, y_train_atk, x_test_atk, y_test_atk = get_tsc_train_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    data_ratio=data_ratio,\n",
    "    data_type=\"atk\"\n",
    ")\n",
    "\n",
    "x_train_atk, y_train_atk, x_test_atk, y_test_atk, enc = preprocess_data(x_train_atk, y_train_atk, x_test_atk, y_test_atk)\n",
    "print(f\"Train data shape: {x_train_atk.shape}, Test data shape: {x_test_atk.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "4c6fab46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 1., 2., 3., 4., 5.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(enc.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b3d564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the parameters of the attacks follow the parameter guide\n",
    "surro_clf_name = \"transformer\" # Chage surrogate model architecture\n",
    "target_clf_name = \"transformer\" # Change the target model architecture\n",
    "y_target = 5 # Change the target class here.\n",
    "amplitude = 0.45 # Change this pattern multiplier the amplitude\n",
    "main_epoch = 38\n",
    "atk_epoch = 2\n",
    "\n",
    "# Others\n",
    "target_epoch = main_epoch - 1\n",
    "\n",
    "# Load configurations\n",
    "training_configs_path = os.path.join(SRC_DIR, \"configs\", \"training_tsc.yaml\")\n",
    "finetune_configs_path = os.path.join(SRC_DIR, \"configs\", \"finetune_surrogate.yaml\")\n",
    "generator_configs_path = os.path.join(SRC_DIR, \"configs\", \"training_generator.yaml\")\n",
    "\n",
    "training_configs = yaml.safe_load(open(training_configs_path, 'r'))\n",
    "finetune_configs = yaml.safe_load(open(finetune_configs_path, 'r'))\n",
    "generator_configs = yaml.safe_load(open(generator_configs_path, 'r'))\n",
    "\n",
    "input_shape = x_train_atk.shape[1:]\n",
    "nb_classes = enc.categories_[0].shape[0] \n",
    "x_total = np.concatenate((x_train_atk, x_test_atk), axis=0)\n",
    "y_total = np.concatenate((y_train_atk, y_test_atk), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c865e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the directories\n",
    "# saved_dir = \"<folder_to_save_results>\"\n",
    "# main_out_dir = os.path.join(saved_dir, dataset_name, \"blackbox_bd\", f\"target_{y_target}_{target_clf_name}\")\n",
    "# target_weight_path = os.path.join(main_out_dir, f\"epoch_{target_epoch}\", \"target_model_update\", \"best_model.keras\")\n",
    "# surro_weight_path = os.path.join(main_out_dir, f\"epoch_{main_epoch}\", f\"surro_ft_epoch_{atk_epoch}\", \"best_model.keras\")\n",
    "# gen_weight_path = os.path.join(main_out_dir, f\"epoch_{main_epoch}\", f\"generator_epoch_{atk_epoch}\", \"best_generator.keras\") # if file is \".weights.h5\", change the extension.\n",
    "\n",
    "# print(f\"Output dir: {main_out_dir}\")\n",
    "# print(f\"Target model weight path: {target_weight_path}\")\n",
    "# print(f\"Surrogate model weight path: {surro_weight_path}\")\n",
    "# print(f\"Generator model weight path: {gen_weight_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4904b4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the directories\n",
    "saved_dir = \"/home/fmg2/v-thanh/Code/results/TSBA/saved_folder\"\n",
    "main_out_dir = os.path.join(saved_dir, dataset_name, \"blackbox_bd\", f\"target_{y_target}_{target_clf_name}\")\n",
    "target_weight_path = os.path.join(main_out_dir, \"target_model.keras\")\n",
    "surro_weight_path = os.path.join(main_out_dir, \"surro_model.keras\")\n",
    "gen_weight_path = os.path.join(main_out_dir, \"best_generator.keras\")\n",
    "\n",
    "print(f\"Output dir: {main_out_dir}\")\n",
    "print(f\"Target model weight path: {target_weight_path}\")\n",
    "print(f\"Surrogate model weight path: {surro_weight_path}\")\n",
    "print(f\"Generator model weight path: {gen_weight_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e151945",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "09a4b42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_clf_train_config = training_configs.get(target_clf_name, {}).get(dataset_name, {})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a22fe91",
   "metadata": {},
   "source": [
    "__Check the inital clean accuracy__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "733e2640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inital_target_wrapper = ClassifierWrapper(\n",
    "#     output_directory=os.path.join(RESULT_DIR, \"tmp_models\"),\n",
    "#     input_shape=input_shape,\n",
    "#     nb_classes=nb_classes,\n",
    "#     training_config=target_clf_train_config,\n",
    "#     clf_name=target_clf_name,\n",
    "#     verbose=True,\n",
    "#     build=True\n",
    "# )\n",
    "# inital_target_model_path = os.path.join(main_out_dir, target_clf_name, \"sp\", \"best_model.keras\")\n",
    "# inital_target_wrapper.model.load_weights(target_weight_path)\n",
    "# inital_target_wrapper.model.trainable = False\n",
    "\n",
    "# # Inital CA\n",
    "# initial_ca = inital_target_wrapper.model.evaluate(x_test_atk, y_test_atk, verbose=0)[1]\n",
    "# print(f\"Initial target model {target_clf_name} accuracy:\", initial_ca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1d055f",
   "metadata": {},
   "source": [
    "__Load the target model classifier__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5a8c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model_wrapper = ClassifierWrapper(\n",
    "    output_directory=os.path.join(RESULT_DIR, \"tmp_models\"),\n",
    "    input_shape=input_shape,\n",
    "    nb_classes=nb_classes,\n",
    "    training_config=target_clf_train_config,\n",
    "    clf_name=target_clf_name,\n",
    "    verbose=True,\n",
    "    build=True\n",
    ")\n",
    "# Check if file existed\n",
    "if not os.path.exists(target_weight_path):\n",
    "    print(f\"File not found: {target_weight_path}\")\n",
    "else:\n",
    "    print(f\"File exists: {target_weight_path}\")\n",
    "target_model_wrapper.model.load_weights(target_weight_path)\n",
    "target_model_wrapper.model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843bd801",
   "metadata": {},
   "source": [
    "__Load the surrogate model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3305419",
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_clf_train_config = training_configs.get(surro_clf_name, {}).get(dataset_name, {})\n",
    "surro_model_wrapper = ClassifierWrapper(\n",
    "    output_directory=os.path.join(RESULT_DIR, \"tmp_models\"),\n",
    "    input_shape=input_shape,\n",
    "    nb_classes=nb_classes,\n",
    "    training_config=surrogate_clf_train_config,\n",
    "    clf_name=surro_clf_name,\n",
    "    verbose=True,\n",
    "    build=True\n",
    ")\n",
    "# Check if file existed\n",
    "if not os.path.exists(surro_weight_path):\n",
    "    print(f\"File not found: {surro_weight_path}\")\n",
    "else:\n",
    "    print(f\"File exists: {surro_weight_path}\")\n",
    "\n",
    "surro_model_wrapper.model.load_weights(surro_weight_path)\n",
    "surro_model_wrapper.model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b32de29",
   "metadata": {},
   "source": [
    "__Load the trigger generator__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f007f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_generator = TriggerGenerator(\n",
    "    output_directory = None,\n",
    "    generator_config = generator_configs[\"blackbox\"][surro_clf_name][dataset_name],\n",
    "    max_amplitude = amplitude,\n",
    "    input_shape = input_shape,\n",
    "    enc = enc\n",
    ")\n",
    "\n",
    "# Check if file existed\n",
    "if not os.path.exists(gen_weight_path):\n",
    "    print(f\"File not found: {gen_weight_path}\")\n",
    "    if gen_weight_path.endswith('.keras'):\n",
    "        gen_weight_path = gen_weight_path.replace('.keras', '.weights.h5')\n",
    "        if not os.path.exists(gen_weight_path):\n",
    "            print(f\"Also not found: {gen_weight_path}\")\n",
    "        else:\n",
    "            print(f\"Found alternative path: {gen_weight_path}\")\n",
    "else:\n",
    "    print(f\"File exists: {gen_weight_path}\")\n",
    "\n",
    "# Load the generator weights\n",
    "if gen_weight_path.endswith('.keras'):\n",
    "    noise_generator.generator.load_weights(gen_weight_path)\n",
    "else: # if file is \"weights.h5\", then it will run this block\n",
    "    # Try loading the whole model instead of just weights\n",
    "    try:\n",
    "        noise_generator.generator.load_weights(gen_weight_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}, load weights instead.\")\n",
    "        loaded_model = keras.models.load_model(gen_weight_path, compile=False)\n",
    "        # Transfer weights layer by layer\n",
    "        for i, layer in enumerate(noise_generator.generator.layers):\n",
    "            if i < len(loaded_model.layers):\n",
    "                try:\n",
    "                    layer.set_weights(loaded_model.layers[i].get_weights())\n",
    "                    print(f\"Successfully loaded weights for layer: {layer.name}\")\n",
    "                except:\n",
    "                    print(f\"Could not load weights for layer: {layer.name}\")\n",
    "noise_generator.generator.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f31929",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "7d1231dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 60ms/step\n",
      "[Target model 'transformer'] ASR: 0.514, CA: 0.972\n",
      "[Surro model 'transformer'] ASR: 0.545, CA: 0.942\n"
     ]
    }
   ],
   "source": [
    "# Now evaluate\n",
    "target_ca = target_model_wrapper.model.evaluate(x_test_atk, y_test_atk, verbose=0)[1]\n",
    "surro_ca = surro_model_wrapper.model.evaluate(x_test_atk, y_test_atk, verbose=0)[1]\n",
    "\n",
    "# Apply trigger to the test set\n",
    "x_triggered = noise_generator.apply_trigger(x_test_atk)\n",
    "y_targets = enc.transform(np.array([y_target]*len(x_triggered)).reshape(-1, 1)).toarray()\n",
    "\n",
    "# Evaluate the target model\n",
    "triggered_dataset = create_dataset(x_triggered, y_targets, batch_size=512, shuffle=False)\n",
    "asr_target = target_model_wrapper.model.evaluate(triggered_dataset, verbose=0)[1]\n",
    "print(f\"[Target model '{target_clf_name}'] ASR: {asr_target:.3f}, CA: {target_ca:.3f}\")\n",
    "\n",
    "# Evaluate the surrogate model\n",
    "asr_surrogate = surro_model_wrapper.model.evaluate(triggered_dataset, verbose=0)[1]\n",
    "print(f\"[Surro model '{surro_clf_name}'] ASR: {asr_surrogate:.3f}, CA: {surro_ca:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
